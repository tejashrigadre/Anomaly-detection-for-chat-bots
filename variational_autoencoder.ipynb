{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d69886ba-fe7b-4fca-a517-4fa0338dca07",
   "metadata": {},
   "source": [
    "## Using Variational Autoencoder for Out-Of-Distribution data detection\n",
    "### A variational autoencoder for input regeration is designed which takes embedded vector as input, performs compression and regerates it as the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6119f2ea-eb88-42ce-bf2f-e158dd380e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all required libraries\n",
    "from keras.layers import Layer, Bidirectional, Dense,Flatten, Embedding, Input, Lambda, LSTM, RepeatVector, TimeDistributed, Layer, Activation, Dropout\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CosineSimilarity\n",
    "from tensorflow.keras.losses import mse\n",
    "from tensorflow.keras.losses import Reduction\n",
    "from tensorflow.keras import optimizers\n",
    "#from keras import backend as K\n",
    "from tensorflow.keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Reshape\n",
    "from scipy import spatial\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import codecs\n",
    "import re\n",
    "import string\n",
    "import math\n",
    "import csv\n",
    "import os\n",
    "import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea396c6-e818-43ac-a248-f25dd526021d",
   "metadata": {},
   "source": [
    "#### The input dataset downloaded from kaggle. This dataset contains 15000 user's spoken queries collected over 150 intent classes, it also contains 1000 out-of-domain (OOD) sentences that are not covered by any of the known classes.\n",
    "\n",
    "OOS - Out-of-scope data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee799f78-d283-4c2e-a639-a21325fe3a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading input jason files into pandas dataframe\n",
    "df_train = pd.read_json('is_train.json')\n",
    "df_val = pd.read_json('is_val.json')\n",
    "df_test = pd.read_json('is_test.json')\n",
    "\n",
    "oos_train = pd.read_json('oos_train.json')\n",
    "oos_val = pd.read_json('oos_val.json')\n",
    "oos_test = pd.read_json('oos_test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7aef9d98-a761-459f-925e-edd6089c68ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15000 entries, 0 to 14999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   0       15000 non-null  object\n",
      " 1   1       15000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 234.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cabfd2e0-d168-4c6a-a5f2-f027619f4daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns = ['question','intent']\n",
    "df_val.columns = ['question','intent']\n",
    "df_test.columns = ['question','intent']\n",
    "\n",
    "oos_train.columns = ['question','intent']\n",
    "oos_val.columns = ['question','intent']\n",
    "oos_test.columns = ['question','intent']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c03496-c3a3-4d94-a240-bd6385b5cd4c",
   "metadata": {},
   "source": [
    "### Data preprocessing :\n",
    "Stop words and punctuatons are removed from all input samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01648766-eb49-45b4-bc2f-e512b526678f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/tejashri/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fede498-1abb-46cd-bf52-9edd5ef6dfd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d203a3ac-94f2-4df3-bb54-0b9c5b789faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stop words\n",
    "df_train['question'] = df_train['question'].apply(lambda x : remove_stopwords(x))\n",
    "df_test['question'] = df_test['question'].apply(lambda x : remove_stopwords(x))\n",
    "df_val['question'] = df_val['question'].apply(lambda x : remove_stopwords(x))\n",
    "\n",
    "oos_train['question'] = oos_train['question'].apply(lambda x : remove_stopwords(x))\n",
    "oos_test['question'] = oos_test['question'].apply(lambda x : remove_stopwords(x))\n",
    "oos_val['question'] = oos_val['question'].apply(lambda x : remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1757826-3b2a-4436-8ebf-a553f0f50b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing punctuations\n",
    "df_train['question'] = df_train['question'].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '' , x))\n",
    "df_test['question'] = df_test['question'].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '' , x))\n",
    "df_val['question'] = df_val['question'].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '' , x))\n",
    "\n",
    "oos_train['question'] = oos_train['question'].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '' , x))\n",
    "oos_test['question'] = oos_test['question'].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '' , x))\n",
    "oos_val['question'] = oos_val['question'].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '' , x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb81478f-d741-49fa-ab02-296a2ac4144c",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_queries = df_train.question\n",
    "oos_queries = oos_train.question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af74019-cc06-44bb-b4b1-05a540ec18e3",
   "metadata": {},
   "source": [
    "### Input text is tokenized using keras preprocessing library and converted into GloVe embedding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5975669a-e1ed-462e-857b-f088ee5bc6a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5144 unique tokens\n",
      "Shape of data tensor: (15000, 8)\n",
      "5145\n"
     ]
    }
   ],
   "source": [
    "#Tokenizing text data\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 8\n",
    "MAX_NB_WORDS = 12000\n",
    "EMBEDDING_DIM = 50\n",
    "GLOVE_EMBEDDING = \"glove/glove.6B.50d.txt\"\n",
    "\n",
    "tokenizer = Tokenizer(MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(in_queries)\n",
    "word_index = tokenizer.word_index #the dict values start from 1 so this is fine with zeropadding\n",
    "index2word = {v: k for k, v in word_index.items()}\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "sequences = tokenizer.texts_to_sequences(in_queries)\n",
    "#print(sequences)\n",
    "data_1 = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH,padding='post')\n",
    "#print(data_1)\n",
    "print('Shape of data tensor:', data_1.shape)\n",
    "NB_WORDS = (min(tokenizer.num_words, len(word_index)) + 1 ) #+1 for zero padding\n",
    "print(NB_WORDS)\n",
    "data_1_val = data_1[801000:807000] #select 6000 sentences as validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c959b887-f85f-42d7-bf74-858c3e350242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 8)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = data_1\n",
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b9ffe2a8-6ab1-4e2d-9bee-e3a43d11f00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "Null word embeddings: 1\n"
     ]
    }
   ],
   "source": [
    "#Creating GloVe embedding matrix for input texts\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(GLOVE_EMBEDDING, encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "glove_embedding_matrix = np.zeros((NB_WORDS, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i < NB_WORDS:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be the word embedding of 'unk'.\n",
    "            glove_embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            glove_embedding_matrix[i] = embeddings_index.get('unk')\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(glove_embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59274cd6-ac81-4804-bcf5-932e2da5c65a",
   "metadata": {},
   "source": [
    "Defining batch-size, maximum sentence length, embedding vector size, latent vector dimesion and intermediate output dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0421ed7b-5fad-4c3f-883f-325ee17c4e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "max_len = MAX_SEQUENCE_LENGTH\n",
    "emb_dim = EMBEDDING_DIM\n",
    "latent_dim = 4\n",
    "intermediate_dim = 256\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6860e5-cbfb-4719-9f87-5abf9f84e390",
   "metadata": {},
   "source": [
    "Word embeddings are generated using keras Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da21412f-ef64-498b-b20f-ff9599816d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-22 00:40:49.267578: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([8, 50])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_embd = Embedding(NB_WORDS, 50, weights=[glove_embedding_matrix],\n",
    "                            input_length=max_len, trainable=False)(data_train)\n",
    "x_embd[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823a9e8d-2355-43fd-a735-062882d53399",
   "metadata": {},
   "source": [
    "### A sampling function is defined to sample vector z from latent space distribution using mean and variance.\n",
    "https://keras.io/examples/generative/vae/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2074460-4c93-4bf0-94d7-5fcb80a0c356",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5185c154-c842-4c00-9fa9-db904ea2a482",
   "metadata": {},
   "source": [
    "### Encoder and decoder model based on image regeneration VAE model and Text Generation VAE examples\n",
    "https://github.com/NicGian/text_VAE\n",
    "\n",
    "https://www.tensorflow.org/tutorials/generative/cvae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "adfab1f4-a8af-4322-a18e-5321697b37d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 8, 50)]      0           []                               \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)                  (None, 256)          314368      ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 256)          0           ['lstm_2[0][0]']                 \n",
      "                                                                                                  \n",
      " x (Dense)                      (None, 256)          65792       ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 256)          0           ['x[0][0]']                      \n",
      "                                                                                                  \n",
      " z_mean (Dense)                 (None, 4)            1028        ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " z_log_var (Dense)              (None, 4)            1028        ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " sampling_1 (Sampling)          (None, 4)            0           ['z_mean[0][0]',                 \n",
      "                                                                  'z_log_var[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 382,216\n",
      "Trainable params: 382,216\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# encoder model\n",
    "inputs = Input(shape=(max_len, 50))\n",
    "h = LSTM(intermediate_dim, return_sequences=False, recurrent_dropout=0.2)(inputs)\n",
    "xd = Dropout(0.2)(h)\n",
    "x1 = tf.keras.layers.Dense(intermediate_dim, name = 'x',activation='linear')(xd)\n",
    "x = Dropout(0.2)(x1)\n",
    "z_mean = tf.keras.layers.Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = tf.keras.layers.Dense(latent_dim, name='z_log_var')(x)\n",
    "\n",
    "# use the reparameterization trick and get the output from the sample() function\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "\n",
    "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d6a95d2a-ff0e-4f84-a835-9be47174158f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " z_sampling (InputLayer)     [(None, 4)]               0         \n",
      "                                                                 \n",
      " repeat_vector_1 (RepeatVect  (None, 8, 4)             0         \n",
      " or)                                                             \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 8, 256)            267264    \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDis  (None, 8, 50)            12850     \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 8, 50)             2550      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 282,664\n",
      "Trainable params: 282,664\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# decoder model\n",
    "repeated_context = RepeatVector(max_len)\n",
    "latent_inputs = Input(shape=(latent_dim), name='z_sampling')\n",
    "decoder_mean = TimeDistributed(Dense(50, activation='linear'))\n",
    "decoder_h = LSTM(intermediate_dim, return_sequences=True, recurrent_dropout=0.2)\n",
    "h_decoded = decoder_h(repeated_context(latent_inputs))\n",
    "x_decoded_mean = decoder_mean(h_decoded)\n",
    "outputs = tf.keras.layers.Dense(50, activation='linear')(x_decoded_mean)\n",
    "\n",
    "# Instantiate the decoder model:\n",
    "decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e07588-e395-4f2c-a6e1-8a203f62e2d9",
   "metadata": {},
   "source": [
    "### Custom train step based on VAE example by F. Chollet to include reconstruction and KL loss.\n",
    "#### Cosine similarity between input embedding vectors and generated embedding vectors is used to calculate reconstruction loss.\n",
    "\n",
    "https://keras.io/examples/generative/vae/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9a43e0dc-2b16-4642-9543-2acc35ea50c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction_loss = CosineSimilarity(axis=1)(data, reconstruction) + 1\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_sum(kl_loss)\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31baff0-1f6b-4c01-a263-03bc5d88934e",
   "metadata": {},
   "source": [
    "### Adam optimizer is used with loss defined in custom train step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e7911055-69ce-453a-b39f-46433e8e0e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optimizers.Adam(learning_rate=0.001, clipvalue=0.5)\n",
    "vae_model = VAE(encoder, decoder)\n",
    "vae_model.compile(optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "77eccabe-3b19-47bd-aba5-d78c12072443",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "235/235 [==============================] - 14s 46ms/step - loss: 0.7588 - reconstruction_loss: 0.7336 - kl_loss: 0.0079\n",
      "Epoch 2/10\n",
      "235/235 [==============================] - 11s 47ms/step - loss: 0.7355 - reconstruction_loss: 0.7323 - kl_loss: 0.0018\n",
      "Epoch 3/10\n",
      "235/235 [==============================] - 12s 49ms/step - loss: 0.7331 - reconstruction_loss: 0.7321 - kl_loss: 2.2586e-04\n",
      "Epoch 4/10\n",
      "235/235 [==============================] - 12s 51ms/step - loss: 0.7309 - reconstruction_loss: 0.7318 - kl_loss: 2.5175e-04\n",
      "Epoch 5/10\n",
      "235/235 [==============================] - 11s 49ms/step - loss: 0.7305 - reconstruction_loss: 0.7318 - kl_loss: 3.1899e-04\n",
      "Epoch 6/10\n",
      "235/235 [==============================] - 11s 49ms/step - loss: 0.7298 - reconstruction_loss: 0.7316 - kl_loss: 1.6824e-04\n",
      "Epoch 7/10\n",
      "235/235 [==============================] - 12s 49ms/step - loss: 0.7316 - reconstruction_loss: 0.7315 - kl_loss: 1.4566e-04\n",
      "Epoch 8/10\n",
      "235/235 [==============================] - 11s 47ms/step - loss: 0.7315 - reconstruction_loss: 0.7315 - kl_loss: 2.1130e-04\n",
      "Epoch 9/10\n",
      "235/235 [==============================] - 14s 60ms/step - loss: 0.7321 - reconstruction_loss: 0.7314 - kl_loss: 1.6817e-04\n",
      "Epoch 10/10\n",
      "235/235 [==============================] - 12s 49ms/step - loss: 0.7302 - reconstruction_loss: 0.7315 - kl_loss: 1.6518e-04\n"
     ]
    }
   ],
   "source": [
    "results = vae_model.fit(x_embd,\n",
    "                        shuffle=True,\n",
    "                        epochs=10,\n",
    "                        batch_size=64\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "33d43a80-b2f8-41fd-8883-c95a7a772a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(results.history['loss'])\n",
    "#plt.plot(results.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right');\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4043ad1c-83d0-4bbe-ad90-edd6ffd1a488",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = df_test.question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc2c5f9-b602-42e4-9f13-3b15572f77eb",
   "metadata": {},
   "source": [
    "### Embedding vectors generated for test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5f3139b9-2a55-434b-ad83-123080dd650f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2672 unique tokens\n",
      "Shape of data tensor: (4500, 8)\n",
      "2673\n"
     ]
    }
   ],
   "source": [
    "tokenizer1 = Tokenizer(MAX_NB_WORDS)\n",
    "tokenizer1.fit_on_texts(data_test)\n",
    "word_index1 = tokenizer1.word_index #the dict values start from 1 so this is fine with zeropadding\n",
    "index2word1 = {v: k for k, v in word_index1.items()}\n",
    "print('Found %s unique tokens' % len(word_index1))\n",
    "sequences1 = tokenizer1.texts_to_sequences(data_test)\n",
    "data_2 = pad_sequences(sequences1, maxlen=MAX_SEQUENCE_LENGTH,padding='post')\n",
    "print('Shape of data tensor:', data_2.shape)\n",
    "NB_WORDS1 = (min(tokenizer1.num_words, len(word_index1)) + 1 ) #+1 for zero padding\n",
    "print(NB_WORDS1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9211cce2-d87d-4de4-b8c7-cc12c2b9e90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "would say fly italian\n",
      "put pto request september 1st september 8th\n"
     ]
    }
   ],
   "source": [
    "print(data_test[0])\n",
    "print(data_test[249])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2a8af49f-90c9-41e9-9719-da68a6d35d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "(2673, 50)\n",
      "Null word embeddings: 1\n"
     ]
    }
   ],
   "source": [
    "embeddings_index1 = {}\n",
    "f = open(GLOVE_EMBEDDING, encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index1[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index1))\n",
    "\n",
    "glove_embedding_matrix1 = np.zeros((NB_WORDS1, 50))\n",
    "print(glove_embedding_matrix1.shape)\n",
    "for word, i in word_index1.items():\n",
    "    if i < NB_WORDS1:\n",
    "        embedding_vector = embeddings_index1.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be the word embedding of 'unk'.\n",
    "            glove_embedding_matrix1[i] = embedding_vector\n",
    "        else:\n",
    "            glove_embedding_matrix1[i] = embeddings_index1.get('unk')\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(glove_embedding_matrix1, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e5a0e963-64fa-4929-a308-8c1d675d07c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4500, 8, 50])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = Embedding(NB_WORDS1, 50, weights=[glove_embedding_matrix1],\n",
    "                            input_length=max_len, trainable=False)(data_2)\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8d50cffd-68e6-4af5-9b76-2496aa0aa8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_,pred_e = vae_model.encoder.predict(x_test)\n",
    "pred = vae_model.decoder.predict(pred_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a3080399-c11c-4ac0-a99d-f4195d7430e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4500, 8, 50)\n"
     ]
    }
   ],
   "source": [
    "print(pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2abb1a8d-4ae9-414b-a35e-834b33fb0b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error_term(v1, v2, _rmse=True):\n",
    "    if _rmse:\n",
    "        return np.sqrt(np.mean((v1 - v2) ** 2, axis=1))\n",
    "    else:\n",
    "        return CosineSimilarity(v1, v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2a3386cd-f4d2-4fb6-bc57-d491c32fc868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average error: 0.5576151609420776\n",
      "Cosine similarity loss when input and output are same : 0.0\n",
      "Cosine similarity loss with output vector : 0.8894054889678955\n"
     ]
    }
   ],
   "source": [
    "mae_vector = get_error_term(pred, x_test, _rmse=True)\n",
    "print(f'Average error: {np.mean(mae_vector)}')\n",
    "cs = (CosineSimilarity(axis=1)(x_embd[1:2], x_embd[1:2]))+ 1\n",
    "print(f'Cosine similarity loss when input and output are same : {cs}')\n",
    "pred_cs = (CosineSimilarity(axis=1)(x_embd[1:2], x_test[1:2]))+ 1 \n",
    "print(f'Cosine similarity loss with output vector : {pred_cs}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1fa144-3e5c-4341-b9a5-a0e7b3f97a59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
